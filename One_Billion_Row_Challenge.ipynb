{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# One Billion Row Challenge\n",
    "\n",
    "Изначально [челендж](https://github.com/gunnarmorling/1brc) предназначен для соревнования программистов на Java. Мне стало интересно попробовать сделать то же самое на _интерактивном_ Python. \n",
    "Основная сложность в этом челендже - работа с памятью. Если создаются большие объекты - их необходимо свопировать на диск и это занимает время. При этом не факт, что ядро интерпретатора не разрушится. "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [],
   "source": [
    "import pandas as pd, random, time, polars as pl\n",
    "from os import cpu_count\n",
    "from joblib import Parallel, delayed \n",
    "from tqdm import tqdm"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 29,
   "metadata": {},
   "outputs": [],
   "source": [
    "#!pip install duckdb\n",
    "import duckdb"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Обработка исходных данных в текстовом формате\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Генерация исходных данных \n",
    "\n",
    "Исходные данные для работы генерируются случайным образом и записываются на диск.  Я решил записывать данные на внешний SSD, подключенный через USB порт. Используется список всех погодных станций в мире, из которого отобрано 10к элементов. "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "(44691, 2)\n"
     ]
    },
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<style scoped>\n",
       "    .dataframe tbody tr th:only-of-type {\n",
       "        vertical-align: middle;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: right;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>City</th>\n",
       "      <th>Temp</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>0</th>\n",
       "      <td>Tokyo</td>\n",
       "      <td>35.6897</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>1</th>\n",
       "      <td>Jakarta</td>\n",
       "      <td>-6.1750</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>2</th>\n",
       "      <td>Delhi</td>\n",
       "      <td>28.6100</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>3</th>\n",
       "      <td>Guangzhou</td>\n",
       "      <td>23.1300</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>4</th>\n",
       "      <td>Mumbai</td>\n",
       "      <td>19.0761</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "</div>"
      ],
      "text/plain": [
       "        City     Temp\n",
       "0      Tokyo  35.6897\n",
       "1    Jakarta  -6.1750\n",
       "2      Delhi  28.6100\n",
       "3  Guangzhou  23.1300\n",
       "4     Mumbai  19.0761"
      ]
     },
     "execution_count": 2,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "stations_path = './data/weather_stations.csv'\n",
    "stations = pd.read_csv(stations_path, sep=';', skiprows=2, header=None, names=['City','Temp'])\n",
    "print(stations.shape)\n",
    "stations.head()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [],
   "source": [
    "text_path = \"/Volumes/Samsung_T5/Data/One Billion Row Challenge/data/measur.txt\"\n",
    "parq_path = \"/Volumes/Samsung_T5/Data/One Billion Row Challenge/data/measur.parquet\""
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {},
   "outputs": [],
   "source": [
    "\n",
    "# Максимально 10 тыс станций для измерения температуры\n",
    "weather_stations_10k = random.choices(list(stations.City), k=10000)\n",
    "coldest_temp = -99.9\n",
    "hottest_temp = 99.9\n",
    "batch_size = 10000 # instead of writing line by line to file, process a batch of stations and put it to disk\n",
    "num_rows_to_create = 1000_000_000\n",
    "num_chunks = num_rows_to_create // batch_size"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Генерация текстового файла. Использую внешний SSD для размещения файла. Алгоритм практически заимствован из соревнования. Маленькое исключение - формирование батча в виде списка и слияние перед записью оказалось чуть быстрее. "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "  0%|          | 0/100000 [00:00<?, ?it/s]"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "100%|██████████| 100000/100000 [15:40<00:00, 106.29it/s]\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "It takes 941.1888129711151 sec to write file\n"
     ]
    }
   ],
   "source": [
    "was = time.time()\n",
    "with open(text_path, 'w') as file:\n",
    "    for n in tqdm(range(num_chunks)):\n",
    "        selection = random.choices(weather_stations_10k, k=batch_size)\n",
    "        batch = [f\"{city};{random.uniform(coldest_temp, hottest_temp):.1f}\\n\" for city in selection]\n",
    "        file.write(''.join(batch))\n",
    "del selection, batch\n",
    "print(f\"It took {time.time()-was} sec to write file\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "It takes 391.10774302482605 sec to read 1000000000 lines.\n"
     ]
    }
   ],
   "source": [
    "was = time.time()\n",
    "cnt = 0\n",
    "with open(text_path, \"r\") as f:\n",
    "    while len(f.readline())>0 : cnt += 1\n",
    "print(f\"It took {time.time()-was} sec to read {cnt} lines.\")    "
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Решение задачи с помощью pandas\n",
    "\n",
    "На моей машинке не очень много оперативной памяти и не удается решить задачу напрямую только методами pandas. Используется вариант кусочной обработки исходного файла:\n",
    "- в память считывается кусок исходного файла в небольшой датафрейм,\n",
    "- рассчитываются промежуточные статистики, и результат выгружается в список,\n",
    "- список сливается в новый датафрейм,\n",
    "- для итогового датафрейма определяются окончательные статистики. \n",
    "\n",
    "Для ускорения работы используем распараллеливание с помощью joblib. "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "metadata": {},
   "outputs": [],
   "source": [
    "chunk_size  = 2_000_000\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 16,
   "metadata": {},
   "outputs": [],
   "source": [
    "def process_chunk(chunk):\n",
    "    res = chunk.groupby(\"City\").aggregate({'Temp':['min','sum', 'count','max']}).reset_index()\n",
    "    res.columns = ['_'.join(c) if len(c[1])>0 else c[0] for c in res.columns]\n",
    "    return res"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "num_cpu = cpu_count()\n",
    "print(f\"Using {num_cpu} cpu cores\")\n",
    "multi_cpu = Parallel(n_jobs=num_cpu)\n",
    "\n",
    "was = time.time()\n",
    "\n",
    "interm = multi_cpu(delayed(process_chunk)(chunk) for chunk in pd.read_csv(text_path, sep=';', header=None, names=['City','Temp'],\n",
    "                  chunksize=chunk_size)) \n",
    "\n",
    "r2 = (pd.concat(interm).groupby(\"City\")\n",
    "      .aggregate({'Temp_min':'min','Temp_sum':'sum', 'Temp_count':'count', 'Temp_max':'max'}).reset_index()\n",
    "      .assign(Temp_mean= lambda x: x.Temp_sum / x.Temp_count)\n",
    "      .drop(columns=['Temp_sum', 'Temp_count']))\n",
    "del multi_cpu, interm\n",
    "print(f\"It took {time.time()-was} sec\")\n",
    "r2.head()\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Удивительным образом, получилось довольно много - 420 секунд на чтение и обработку входного файла.  Похоже, что основным ограничителем здесь выступает процесс чтения куска данных методом `read_csv`."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Решение задачи с помощью polars\n",
    "\n",
    "Polars - актуальная библиотека для обработки больших массивов данных, написанная целиком на Rust. Ее отличительной особенностью является ленивая обработка входного файла, которая осуществляется параллельно, и только по специальной команде. \n",
    "     \n",
    "Обработка файла большого объема требует использования двух нюансов: \n",
    "- необходимо в явном виде указать размер куска данных, обрабатываемого единовременно, так как встроенная формула слишком оптимистична;\n",
    "- необходимо в явном виде указать кусочную обработку запросов, так как по умолчанию polars стремится прочесть все данные в память и делать запрос один раз, что приводит к проблемам памяти. \n",
    "\n",
    "В разработке решения мне помог https://github.com/lvgalvao/One-Billion-Row-Challenge-Python "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "It takes 81.5114197731018 sec\n"
     ]
    },
    {
     "data": {
      "text/html": [
       "<div><style>\n",
       ".dataframe > thead > tr,\n",
       ".dataframe > tbody > tr {\n",
       "  text-align: right;\n",
       "  white-space: pre-wrap;\n",
       "}\n",
       "</style>\n",
       "<small>shape: (5, 4)</small><table border=\"1\" class=\"dataframe\"><thead><tr><th>City</th><th>Temp_min</th><th>Temp_mean</th><th>Temp_max</th></tr><tr><td>str</td><td>f64</td><td>f64</td><td>f64</td></tr></thead><tbody><tr><td>&quot;A Yun Pa&quot;</td><td>-99.9</td><td>-0.10654</td><td>99.9</td></tr><tr><td>&quot;Aabenraa&quot;</td><td>-99.9</td><td>-0.32684</td><td>99.9</td></tr><tr><td>&quot;Aarau&quot;</td><td>-99.9</td><td>-0.104795</td><td>99.9</td></tr><tr><td>&quot;Abadla&quot;</td><td>-99.9</td><td>0.004239</td><td>99.9</td></tr><tr><td>&quot;Abadou&quot;</td><td>-99.9</td><td>0.033001</td><td>99.9</td></tr></tbody></table></div>"
      ],
      "text/plain": [
       "shape: (5, 4)\n",
       "┌──────────┬──────────┬───────────┬──────────┐\n",
       "│ City     ┆ Temp_min ┆ Temp_mean ┆ Temp_max │\n",
       "│ ---      ┆ ---      ┆ ---       ┆ ---      │\n",
       "│ str      ┆ f64      ┆ f64       ┆ f64      │\n",
       "╞══════════╪══════════╪═══════════╪══════════╡\n",
       "│ A Yun Pa ┆ -99.9    ┆ -0.10654  ┆ 99.9     │\n",
       "│ Aabenraa ┆ -99.9    ┆ -0.32684  ┆ 99.9     │\n",
       "│ Aarau    ┆ -99.9    ┆ -0.104795 ┆ 99.9     │\n",
       "│ Abadla   ┆ -99.9    ┆ 0.004239  ┆ 99.9     │\n",
       "│ Abadou   ┆ -99.9    ┆ 0.033001  ┆ 99.9     │\n",
       "└──────────┴──────────┴───────────┴──────────┘"
      ]
     },
     "execution_count": 8,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "#Фиксирование размера одновременно обрабатываемого куска данных\n",
    "pl.Config.set_streaming_chunk_size(chunk_size)  \n",
    "\n",
    "was = time.time()\n",
    "res = (pl.scan_csv(text_path, separator=';', has_header=False, new_columns=[\"City\", \"Temp\"],  \n",
    "                  schema={\"City\": pl.String, \"Temp\": pl.Float64}, low_memory=True)\n",
    "                  .group_by('City')\n",
    "                  .agg(pl.col('Temp').min().name.suffix('_min'), pl.col('Temp').mean().name.suffix('_mean'), pl.col('Temp').max().name.suffix('_max'))\n",
    "                  .sort('City')\n",
    "                  .collect(streaming=True))  #кусочная обработка!\n",
    "print(f\"It took {time.time()-was} sec\")\n",
    "res.head(5)\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Чтение и обработка файла c помощью polars заняли намного меньше времени - 82 сек.  Выигрыш примерно в 6 раз.\n",
    "Но polars очень чувствителен к формату данных. У меня была версия кода, генерировавшая файл с пробелом перед окончанием строки. По непонятной причине polars не читал такой файл кусками и была проблема с работоспособностью кода. \n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Обработка данных с помощью DuckDB\n",
    "\n",
    "DuckDB - это еще одна популярная библиотека для выполнения аналитических расчетов. Она позволяет обращаться к различным источникам данных для проведения операций аналитической свертки и группировки. В рамках этого анализа попробую использовать это решение сначала на текстовых данных. В разработке решения мне помог https://github.com/lvgalvao/One-Billion-Row-Challenge-Python "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 31,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "ab278cb37b234c889d01ba276e44b2e5",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "FloatProgress(value=0.0, layout=Layout(width='auto'), style=ProgressStyle(bar_color='black'))"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "It took 48.0512969493866 sec\n"
     ]
    },
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<style scoped>\n",
       "    .dataframe tbody tr th:only-of-type {\n",
       "        vertical-align: middle;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: right;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>city</th>\n",
       "      <th>min_temperature</th>\n",
       "      <th>mean_temperature</th>\n",
       "      <th>max_temperature</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>0</th>\n",
       "      <td>A Yun Pa</td>\n",
       "      <td>-99.9</td>\n",
       "      <td>-0.1</td>\n",
       "      <td>99.9</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>1</th>\n",
       "      <td>Aabenraa</td>\n",
       "      <td>-99.9</td>\n",
       "      <td>-0.3</td>\n",
       "      <td>99.9</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>2</th>\n",
       "      <td>Aarau</td>\n",
       "      <td>-99.9</td>\n",
       "      <td>-0.1</td>\n",
       "      <td>99.9</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>3</th>\n",
       "      <td>Abadla</td>\n",
       "      <td>-99.9</td>\n",
       "      <td>0.0</td>\n",
       "      <td>99.9</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>4</th>\n",
       "      <td>Abadou</td>\n",
       "      <td>-99.9</td>\n",
       "      <td>0.0</td>\n",
       "      <td>99.9</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "</div>"
      ],
      "text/plain": [
       "       city  min_temperature  mean_temperature  max_temperature\n",
       "0  A Yun Pa            -99.9              -0.1             99.9\n",
       "1  Aabenraa            -99.9              -0.3             99.9\n",
       "2     Aarau            -99.9              -0.1             99.9\n",
       "3    Abadla            -99.9               0.0             99.9\n",
       "4    Abadou            -99.9               0.0             99.9"
      ]
     },
     "execution_count": 31,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "query = \"\"\"\n",
    "        SELECT city,\n",
    "            MIN(temperature) AS min_temperature,\n",
    "            CAST(AVG(temperature) AS DECIMAL(3,1)) AS mean_temperature,\n",
    "            MAX(temperature) AS max_temperature\n",
    "        FROM read_csv(\"/Volumes/Samsung_T5/Data/One Billion Row Challenge/data/measur.txt\", \n",
    "                    AUTO_DETECT=FALSE, sep=';', columns={'city':VARCHAR, 'temperature': 'DECIMAL(3,1)'})\n",
    "        GROUP BY city\n",
    "        ORDER BY city\n",
    "\"\"\"\n",
    "was = time.time()\n",
    "df = duckdb.sql(query).df()\n",
    "print(f\"It took {time.time()-was} sec\")\n",
    "df.head()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Обработка данных с помощью DuckDB заняла в случае текстового файла 48 секунд.  Это рекорд для текстовых файлов."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "\n",
    "Обычный челендж на этом заканчивается.  Но у меня возникла идея попробовать все то же самое с помощью файлов parquet"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Обработка файлов parquet"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Генерация файла parquet c помощью fastparquet\n",
    "\n",
    "Для генерации файла parquet можно использовать две библиотеки - fastparquet и pyarrow. Я сравнил оба варианта. Первоначальный вариант был на основе fastparquet. В результате нескольких итераций я решил генерировать куски (датафреймы) достаточно большого размера и дописывать их в один и тот же файл. Для ускорения генерации температур использован numpy. \n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {},
   "outputs": [],
   "source": [
    "import fastparquet as fp\n",
    "import numpy as np"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "100%|██████████| 100/100 [08:40<00:00,  5.21s/it]\n"
     ]
    }
   ],
   "source": [
    "was = time.time()\n",
    "double_range = hottest_temp - coldest_temp\n",
    "batch_size = 10_000_000\n",
    "num_chunks = num_rows_to_create // batch_size\n",
    "\n",
    "for i in tqdm(range(num_chunks)): #tqdm(range(10)): \n",
    "    selection = random.choices(weather_stations_10k, k=batch_size)\n",
    "    temp = np.random.rand(batch_size) * double_range\n",
    "    temp = temp + coldest_temp\n",
    "    #for _ in range(batch_size):\n",
    "    #    temp.append(random.uniform(coldest_temp, hottest_temp))\n",
    "    df = pd.DataFrame({\"City\":selection, \"Temp\":np.around(temp, decimals=1)})\n",
    "    if i < 1: fp.write(parq_path, df, append=False)\n",
    "    else: fp.write(parq_path, df, append=True)\n",
    "print(f\"It took {time.time()-was} sec\")\n",
    "del selection, temp\n",
    "pf = fp.ParquetFile(parq_path)\n",
    "print(f\"Total {pf.count()} lines and {len(pf.row_groups)} row groups in file\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Генерация файла данных с помомощью fast_parquet заняла в два раза меньше времени, но его объем получился больше чем у текстового файла - 21.3 GB. Вероятно есть еще какие-то тонкости. Внутри файла есть 100 групп строк и можно попробовать итерировать прямо по этим \"естественным\" группам строк. "
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Обработка fast parquet в pandas\n",
    "\n",
    "Pandas из коробки допускает только чтение parquet файла целиком. Вероятно предполагается, что опытные пользователи используют возможности fast_parquet для итеративного чтения и последующей обработки файла.  В данном случае я решил не использовать многопроцессорной обработки. "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 18,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "100%|██████████| 100/100 [06:28<00:00,  3.88s/it]\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "It takes 389.27612805366516 sec\n"
     ]
    },
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<style scoped>\n",
       "    .dataframe tbody tr th:only-of-type {\n",
       "        vertical-align: middle;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: right;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>City</th>\n",
       "      <th>Temp_min</th>\n",
       "      <th>Temp_max</th>\n",
       "      <th>Temp_mean</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>0</th>\n",
       "      <td>A Yun Pa</td>\n",
       "      <td>-99.9</td>\n",
       "      <td>99.9</td>\n",
       "      <td>-33.057</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>1</th>\n",
       "      <td>Aabenraa</td>\n",
       "      <td>-99.9</td>\n",
       "      <td>99.9</td>\n",
       "      <td>177.683</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>2</th>\n",
       "      <td>Aalst</td>\n",
       "      <td>-99.9</td>\n",
       "      <td>99.9</td>\n",
       "      <td>-28.278</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>3</th>\n",
       "      <td>Aalten</td>\n",
       "      <td>-99.9</td>\n",
       "      <td>99.9</td>\n",
       "      <td>50.917</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>4</th>\n",
       "      <td>Abaji</td>\n",
       "      <td>-99.9</td>\n",
       "      <td>99.9</td>\n",
       "      <td>-129.605</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "</div>"
      ],
      "text/plain": [
       "       City  Temp_min  Temp_max  Temp_mean\n",
       "0  A Yun Pa     -99.9      99.9    -33.057\n",
       "1  Aabenraa     -99.9      99.9    177.683\n",
       "2     Aalst     -99.9      99.9    -28.278\n",
       "3    Aalten     -99.9      99.9     50.917\n",
       "4     Abaji     -99.9      99.9   -129.605"
      ]
     },
     "execution_count": 18,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "was = time.time()\n",
    "pf = fp.ParquetFile(parq_path)\n",
    "num_rg = len(pf.row_groups)\n",
    "interm = []\n",
    "for df in tqdm(pf.iter_row_groups(), total=num_rg):\n",
    "    interm.append(process_chunk(df))\n",
    "\n",
    "r2 = (pd.concat(interm).groupby(\"City\")\n",
    "      .aggregate({'Temp_min':'min','Temp_sum':'sum', 'Temp_count':'count', 'Temp_max':'max'}).reset_index()\n",
    "      .assign(Temp_mean= lambda x: x.Temp_sum / x.Temp_count)\n",
    "      .drop(columns=['Temp_sum', 'Temp_count']))\n",
    "del interm\n",
    "print(f\"It took {time.time()-was} sec\")\n",
    "r2.head()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Вариант с чтением и обработкой большого parquet файла занял 389 сек. "
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Генерация файла в pyarrow\n",
    "\n",
    "Основная библиотека для работы с parquet - pyarrow. В данном случае использован тот же самый алгоритм генерации данных."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 19,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "100%|██████████| 100/100 [06:51<00:00,  4.11s/it]"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "It takes 411.12199091911316 sec\n",
      "Total 1000000000 lines and 1000 row groups in file\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "\n"
     ]
    }
   ],
   "source": [
    "import pyarrow as pa, pyarrow.parquet as pq\n",
    "\n",
    "double_range = hottest_temp - coldest_temp\n",
    "batch_size = 10_000_000\n",
    "num_chunks = num_rows_to_create // batch_size\n",
    "parq_path = \"/Volumes/Samsung_T5/Data/One Billion Row Challenge/data/measur2.parquet\"\n",
    "pqwriter = None\n",
    "\n",
    "was = time.time()\n",
    "\n",
    "for i in tqdm(range(num_chunks)): #tqdm(range(10)): \n",
    "    selection = random.choices(weather_stations_10k, k=batch_size)\n",
    "    temp = np.random.rand(batch_size) * double_range\n",
    "    temp = temp + coldest_temp\n",
    "    table = pa.Table.from_pandas(pd.DataFrame({\"City\":selection, \"Temp\":np.around(temp, decimals=1)}))\n",
    "    if i < 1: pqwriter = pq.ParquetWriter(parq_path, table.schema)            \n",
    "    pqwriter.write_table(table)\n",
    "\n",
    "# close the parquet writer\n",
    "if pqwriter: pqwriter.close()\n",
    "        \n",
    "print(f\"It took {time.time()-was} sec\")\n",
    "del selection, temp, table\n",
    "pf = fp.ParquetFile(parq_path)\n",
    "print(f\"Total {pf.count()} lines and {len(pf.row_groups)} row groups in file\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Библиотека pyarrow работает чуть быстрее: 411 секунд вместо 520 секунд, и главное, формирует файл размером в семь раз меньше - 3.23 GB вместо 21.4 GB. \n",
    "\n",
    "Еще одна важная особенность - размер группы строк. Библиотека pyarrow использует размер 2**20 для формирования групп строк и каждая крайняя группа занимает меньше места. "
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Обработка pyarrow parquet в pandas\n",
    "\n",
    "В данном случае читается группа строк в собственный формат pyarrow, а затем она конвертируется в DataFrame. В остальном алгоритм не отличается от реализованного выше. "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 24,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "100%|██████████| 1000/1000 [03:53<00:00,  4.28it/s]\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "It takes 237.1843011379242 sec\n"
     ]
    },
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<style scoped>\n",
       "    .dataframe tbody tr th:only-of-type {\n",
       "        vertical-align: middle;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: right;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>City</th>\n",
       "      <th>Temp_min</th>\n",
       "      <th>Temp_max</th>\n",
       "      <th>Temp_mean</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>0</th>\n",
       "      <td>A Yun Pa</td>\n",
       "      <td>-99.9</td>\n",
       "      <td>99.9</td>\n",
       "      <td>-9.8999</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>1</th>\n",
       "      <td>Aabenraa</td>\n",
       "      <td>-99.9</td>\n",
       "      <td>99.9</td>\n",
       "      <td>14.2728</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>2</th>\n",
       "      <td>Aalst</td>\n",
       "      <td>-99.9</td>\n",
       "      <td>99.9</td>\n",
       "      <td>16.7945</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>3</th>\n",
       "      <td>Aalten</td>\n",
       "      <td>-99.9</td>\n",
       "      <td>99.9</td>\n",
       "      <td>42.8011</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>4</th>\n",
       "      <td>Abaji</td>\n",
       "      <td>-99.9</td>\n",
       "      <td>99.9</td>\n",
       "      <td>6.7245</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "</div>"
      ],
      "text/plain": [
       "       City  Temp_min  Temp_max  Temp_mean\n",
       "0  A Yun Pa     -99.9      99.9    -9.8999\n",
       "1  Aabenraa     -99.9      99.9    14.2728\n",
       "2     Aalst     -99.9      99.9    16.7945\n",
       "3    Aalten     -99.9      99.9    42.8011\n",
       "4     Abaji     -99.9      99.9     6.7245"
      ]
     },
     "execution_count": 24,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "was = time.time()\n",
    "parquet_file = pq.ParquetFile(parq_path)\n",
    "interm = []\n",
    "for i in tqdm(range(parquet_file.num_row_groups)):\n",
    "    interm.append(process_chunk(parquet_file.read_row_group(i).to_pandas()))\n",
    "parquet_file.close()\n",
    "r2 = (pd.concat(interm).groupby(\"City\")\n",
    "      .aggregate({'Temp_min':'min','Temp_sum':'sum', 'Temp_count':'count', 'Temp_max':'max'}).reset_index()\n",
    "      .assign(Temp_mean= lambda x: x.Temp_sum / x.Temp_count)\n",
    "      .drop(columns=['Temp_sum', 'Temp_count']))\n",
    "del interm\n",
    "print(f\"It took {time.time()-was} sec\")\n",
    "r2.head()\n",
    "    "
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Результат в 237 секунд при чтении меньшего по объему файла порадовал.  Возникло желание попробовать сделать обработку на нескольких cpu с помощью joblib, как это было опробовано выше. "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 25,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Using 8 cpu cores\n",
      "It takes 171.3538670539856 sec\n"
     ]
    },
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<style scoped>\n",
       "    .dataframe tbody tr th:only-of-type {\n",
       "        vertical-align: middle;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: right;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>City</th>\n",
       "      <th>Temp_min</th>\n",
       "      <th>Temp_max</th>\n",
       "      <th>Temp_mean</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>0</th>\n",
       "      <td>A Yun Pa</td>\n",
       "      <td>-99.9</td>\n",
       "      <td>99.9</td>\n",
       "      <td>-9.8999</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>1</th>\n",
       "      <td>Aabenraa</td>\n",
       "      <td>-99.9</td>\n",
       "      <td>99.9</td>\n",
       "      <td>14.2728</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>2</th>\n",
       "      <td>Aalst</td>\n",
       "      <td>-99.9</td>\n",
       "      <td>99.9</td>\n",
       "      <td>16.7945</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>3</th>\n",
       "      <td>Aalten</td>\n",
       "      <td>-99.9</td>\n",
       "      <td>99.9</td>\n",
       "      <td>42.8011</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>4</th>\n",
       "      <td>Abaji</td>\n",
       "      <td>-99.9</td>\n",
       "      <td>99.9</td>\n",
       "      <td>6.7245</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "</div>"
      ],
      "text/plain": [
       "       City  Temp_min  Temp_max  Temp_mean\n",
       "0  A Yun Pa     -99.9      99.9    -9.8999\n",
       "1  Aabenraa     -99.9      99.9    14.2728\n",
       "2     Aalst     -99.9      99.9    16.7945\n",
       "3    Aalten     -99.9      99.9    42.8011\n",
       "4     Abaji     -99.9      99.9     6.7245"
      ]
     },
     "execution_count": 25,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "was = time.time()\n",
    "parquet_file = pq.ParquetFile(parq_path)\n",
    "num_cpu = cpu_count()\n",
    "print(f\"Using {num_cpu} cpu cores\")\n",
    "multi_cpu = Parallel(n_jobs=num_cpu)\n",
    "\n",
    "interm = multi_cpu(delayed(process_chunk)(parquet_file.read_row_group(i).to_pandas()) for i in range(parquet_file.num_row_groups))\n",
    "\n",
    "parquet_file.close()\n",
    "\n",
    "r2 = (pd.concat(interm).groupby(\"City\")\n",
    "      .aggregate({'Temp_min':'min','Temp_sum':'sum', 'Temp_count':'count', 'Temp_max':'max'}).reset_index()\n",
    "      .assign(Temp_mean= lambda x: x.Temp_sum / x.Temp_count)\n",
    "      .drop(columns=['Temp_sum', 'Temp_count']))\n",
    "del interm\n",
    "print(f\"It took {time.time()-was} sec\")\n",
    "r2.head()\n",
    "    "
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Ускорение до 171 секунды порадовало. Но результату все равно далеко до polars на текстовых данных. "
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Обработка parquet на polars"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 27,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "It takes 39.408097982406616 sec\n"
     ]
    },
    {
     "data": {
      "text/html": [
       "<div><style>\n",
       ".dataframe > thead > tr,\n",
       ".dataframe > tbody > tr {\n",
       "  text-align: right;\n",
       "  white-space: pre-wrap;\n",
       "}\n",
       "</style>\n",
       "<small>shape: (5, 4)</small><table border=\"1\" class=\"dataframe\"><thead><tr><th>City</th><th>Temp_min</th><th>Temp_mean</th><th>Temp_max</th></tr><tr><td>str</td><td>f64</td><td>f64</td><td>f64</td></tr></thead><tbody><tr><td>&quot;A Yun Pa&quot;</td><td>-99.9</td><td>-0.099345</td><td>99.9</td></tr><tr><td>&quot;Aabenraa&quot;</td><td>-99.9</td><td>0.071339</td><td>99.9</td></tr><tr><td>&quot;Aalst&quot;</td><td>-99.9</td><td>0.168172</td><td>99.9</td></tr><tr><td>&quot;Aalten&quot;</td><td>-99.9</td><td>0.429204</td><td>99.9</td></tr><tr><td>&quot;Abaji&quot;</td><td>-99.9</td><td>0.033718</td><td>99.9</td></tr></tbody></table></div>"
      ],
      "text/plain": [
       "shape: (5, 4)\n",
       "┌──────────┬──────────┬───────────┬──────────┐\n",
       "│ City     ┆ Temp_min ┆ Temp_mean ┆ Temp_max │\n",
       "│ ---      ┆ ---      ┆ ---       ┆ ---      │\n",
       "│ str      ┆ f64      ┆ f64       ┆ f64      │\n",
       "╞══════════╪══════════╪═══════════╪══════════╡\n",
       "│ A Yun Pa ┆ -99.9    ┆ -0.099345 ┆ 99.9     │\n",
       "│ Aabenraa ┆ -99.9    ┆ 0.071339  ┆ 99.9     │\n",
       "│ Aalst    ┆ -99.9    ┆ 0.168172  ┆ 99.9     │\n",
       "│ Aalten   ┆ -99.9    ┆ 0.429204  ┆ 99.9     │\n",
       "│ Abaji    ┆ -99.9    ┆ 0.033718  ┆ 99.9     │\n",
       "└──────────┴──────────┴───────────┴──────────┘"
      ]
     },
     "execution_count": 27,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "#Фиксирование размера одновременно обрабатываемого куска данных\n",
    "chunk_size  = 2_000_000\n",
    "pl.Config.set_streaming_chunk_size(chunk_size)  \n",
    "\n",
    "was = time.time()\n",
    "res = (pl.scan_parquet(parq_path)\n",
    "                  .group_by('City')\n",
    "                  .agg(pl.col('Temp').min().name.suffix('_min'), pl.col('Temp').mean().name.suffix('_mean'), pl.col('Temp').max().name.suffix('_max'))\n",
    "                  .sort('City')\n",
    "                  .collect(streaming=True))\n",
    "print(f\"It took {time.time()-was} sec\")\n",
    "res.head(5)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Этот результат в 39.4 секунды почти вне конкуренции. \n",
    "\n",
    "NB: В процессе работы над этим блокнотом я пробовал использовать polars для записи файла. Polars сделал файл размером 17.7 GB и при этом затем его обрабатывал в течение 1000 секунд.  Вероятно имеет смысл сохранять файлы из polars с преобразованием в pandas, а затем pyarrow table. "
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Обработка файла parquet в DuckDB\n",
    "\n",
    "Попробуем запустить DuckDB поверх файла parquet."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 33,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "2a3ce3e566274001aa012c6711558a55",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "FloatProgress(value=0.0, layout=Layout(width='auto'), style=ProgressStyle(bar_color='black'))"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "It took 26.14646887779236 sec\n"
     ]
    },
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<style scoped>\n",
       "    .dataframe tbody tr th:only-of-type {\n",
       "        vertical-align: middle;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: right;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>City</th>\n",
       "      <th>min_temp</th>\n",
       "      <th>mean_temp</th>\n",
       "      <th>max_temp</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>0</th>\n",
       "      <td>A Yun Pa</td>\n",
       "      <td>-99.9</td>\n",
       "      <td>-0.1</td>\n",
       "      <td>99.9</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>1</th>\n",
       "      <td>Aabenraa</td>\n",
       "      <td>-99.9</td>\n",
       "      <td>0.1</td>\n",
       "      <td>99.9</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>2</th>\n",
       "      <td>Aalst</td>\n",
       "      <td>-99.9</td>\n",
       "      <td>0.2</td>\n",
       "      <td>99.9</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>3</th>\n",
       "      <td>Aalten</td>\n",
       "      <td>-99.9</td>\n",
       "      <td>0.4</td>\n",
       "      <td>99.9</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>4</th>\n",
       "      <td>Abaji</td>\n",
       "      <td>-99.9</td>\n",
       "      <td>0.0</td>\n",
       "      <td>99.9</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "</div>"
      ],
      "text/plain": [
       "       City  min_temp  mean_temp  max_temp\n",
       "0  A Yun Pa     -99.9       -0.1      99.9\n",
       "1  Aabenraa     -99.9        0.1      99.9\n",
       "2     Aalst     -99.9        0.2      99.9\n",
       "3    Aalten     -99.9        0.4      99.9\n",
       "4     Abaji     -99.9        0.0      99.9"
      ]
     },
     "execution_count": 33,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "query = \"\"\"\n",
    "        SELECT city,\n",
    "            MIN(Temp) AS min_temp,\n",
    "            CAST(AVG(Temp) AS DECIMAL(3,1)) AS mean_temp,\n",
    "            MAX(Temp) AS max_temp\n",
    "        FROM \"/Volumes/Samsung_T5/Data/One Billion Row Challenge/data/measur2.parquet\"\n",
    "        GROUP BY city\n",
    "        ORDER BY city\n",
    "\"\"\"\n",
    "was = time.time()\n",
    "df = duckdb.sql(query).df()\n",
    "print(f\"It took {time.time()-was} sec\")\n",
    "df.head()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Результат в 26 секунд выглядит ошеломительным. "
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Итоги\n",
    "\n",
    "- Для работы с файлами большого объема желательно использовать parquet, причем желательно сохранять естественный формат данных pyarrow с помощью библиотеки pyarrow.  Fastparquet больше не fast. \n",
    "\n",
    "- Файл parquet допускает дозапись данных. Можно большой массив данных, например продажи всех товаров розничной сети за несколько лет, записать в виде файла parquet, а затем и работать уже с ним.\n",
    "\n",
    "- Для быстрого знакомства с csv файлом имеет смысл читать его с помощью duckdb. \n",
    "\n",
    "- Pandas может работать с большими данными при использовании кусочной обработки, для которой возможно понадобится использовать стороннюю библиотеку.  Распараллеливание (например с помощью joblib) возможно, но сильно большого выигрыша не дает - последовательное чтение файла является узким горлом. \n",
    "\n",
    "- Еще одна проблема обработки больших файлов на pandas - большое количество избыточного кода. Мне понадобилось 21 строка для кодирования всего процесса и это формирует большое поле для ошибки. \n",
    "\n",
    "- Polars работает быстрее чем pandas и на текстовых данных и на parquet файле. Ускорение составило 6 и 4 раза на разных входных форматах. Код уложился в 10 строк (сокращение в 2 раза) и намного более читаемый.  \n",
    "\n",
    "- DuckDB оказалась самым быстрым решением и для текстовых данных и для бинарных в формате parquet. Размер программы примерно сопоставим с polars, но использует два языка - язык запросов SQL и код Python. Выгода в том, что не надо запоминать Polars API -- надо практиковаться в SQL запросах. \n",
    "\n",
    "- Сравнительного графика времен не будет - я \"покупаю\", а не \"продаю\"."
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": ".venv",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.11.1"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
